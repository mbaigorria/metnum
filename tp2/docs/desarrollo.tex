\section{PageRank}

\subsection{La motivación original: páginas web}

El algoritmo PageRank fue ideado en un principio para buscar una medida de relevancia con el objetivo de ordenar consultas sobre el grafo de la web. El mismo explota la estructura de este grafo, y como veremos mas adelante luego de explicar la idea general del algoritmo, se calcula buscando el autovector de norma unitaria que resuelve un sistema de la forma $Px = x$ con $P\in \mathbb{R}^{n \times n}$, es decir, el autovector asociado al autovalor 1. A su vez, existen dos interpretaciones equivalentes de este modelo, que serán expuestas a continuación.

\subsubsection{Modelado}

El problema se modela a partir de un grafo $G(Web,Links)$ donde $Web$ es el conjunto de sitios web y $Links$ es la cantidad de conexiones entre sitios. Consideremos que toda página web $u \in Web$ esta representada por un vértice y la relación entre páginas por un link con una arista. Una representación posible del grafo es mediante matrices de adyacencia. Definimos la matriz de adyacencia o conectividad $W \in \{0,1\}^{n \times n}$ de forma tal que $w_{ij} = 1$ si la página $j$ tiene un link a la página $i$ y $w_{ij} = 0$ en caso contrario. Por lo tanto, la cantidad de páginas a las que la página $u$ apunta ($d_{out}(u)$) se puede calcular como $n_j = \sum_{i=1}^{n} w_{ij}$.

\subsubsection{Propiedades}

Sea $x_j$ el puntaje asignado a la página o vértice $j \in Web$ y sea otra página $u \in Web$. La idea es buscar una medida de relevancia que cumpla con las siguientes propiedades:
\begin{itemize}
  \item La relevancia de todo sitio web es positiva.
  \item La relevancia de un sitio web debe aumentar a medida que mas sitios únicos lo apuntan.
  \item La relevancia derivada de otro sitio web debe depender de su propia relevancia. Es decir, es mas valioso que me linkee un sitio relevante que uno no relevante. En caso de no cumplirse esta propiedad, el ranking seria fácilmente manipulable al permitir que un usuario cree muchos sitios que linkeen a uno para darle relevancia.
  \item La relevancia de todos los sitios web debe sumar uno. De esta manera estamos ante una distribución de probabilidad de los sitios. Esto nos permite a su vez utilizar muchos teoremas estudiados en procesos estocásticos. Mas adelante veremos que al interpretar esto mediante Cadenas de Markov existe una interpretación directa: la relevancia se puede ver como la proporción del tiempo total que un usuario pasa en ese sitio.
\end{itemize} 

Por lo tanto, estamos buscando una medida de relevancia tal que la importancia obtenida por la página $u$ obtenida por el link de la página $v$ sea proporcional a la relevancia de $v$ e inversamente proporcional al grado de $v$. El aporte del link de $v$ a $u$ entonces es $x_u = x_v / n_v$. Luego, sea $L_k \subseteq Web$ el conjunto de páginas que tienen un link a la página $k$. Por lo tanto, la relevancia total de un sitio sera:

\begin{eqnarray}
x_k = \sum_{j \in L_k} \frac{x_j}{n_j},~~~~k = 1,\dots,n. \label{eq:basicmodel}
\end{eqnarray}

Notar que esta es de cierta manera una definición autoreferencial. La relevancia de un sitio $u$ puede depender de la relevancia de un sitio $v$, y luego la de $v$ puede depender de la de $u$. A priori calcular la relevancia de un sitio puede parecer sumamente complicado, pero luego veremos que al plantearlo como un sistema de ecuaciones esta dificultad per se ya no se presenta.

Definimos entonces una matriz de transición o adyacencia con pesos en las aristas $P \in \mathbb{R}^{n \times n}$ tal que $p_{ij} = 1 / n_j$ si $w_{ij} = 1$ y $p_{ij} = 0$ en caso contrario. Luego, el modelo planteado en (\ref{eq:basicmodel}) para toda página web se puede expresar $Px = x$ donde $x \in \mathbb{R}^n$. Notar que esto es equivalente a encontrar el autovector de autovalor 1 tal que $x_i > 0$ y $\sum_{i=1}^{n} x_i = 1$. Notar que si logramos probar que bajo ciertas condiciones nuestra matriz de transición tiene autovalor 1, el signo de todos los elementos de un autovector es el mismo y la dimension del autoespacio es 1 ya tenemos un ranking valido. Esto se debe a que cualquier autovector que cumple con estas propiedades puede ser reescalado a uno de norma unitaria con $x_i \geq 0$.

\pagebreak

\subsubsection{Existencia y Unicidad}

Bryan y Leise \cite{Bryan2006} analiza y prueba las condiciones bajo las que podemos garantizar que:
\begin{itemize}
\item La matriz de transición tiene autovalor 1.
\item La dimension del autoespacio asociado al autovalor 1 es 1. Es deseable que el ranking asociado a una matriz de transición sea único.
\item El signo de todos los elementos del autovector asociado al autovalor 1 es el mismo.
\end{itemize}

Veamos bajo que condiciones nuestra matriz de transición cumple con estas propiedades:

\begin{definition}
Una matriz cuadrada se llama estocástica por columnas si todos sus elementos son positivos y la suma de cada columna es igual a 1.
\end{definition}

A partir de esta definición se puede probar la siguiente proposición:
\begin{proposition}
Toda matriz estocástica por columnas tiene a 1 como autovalor.
\end{proposition}

Esto significa que si no existen \texttt{dangling nodes}, es decir, vértices con $d_{out} = 0$, podemos garantizar que nuestra matriz de transición es estocástica por columnas.

Notar que bajo las condiciones actuales no podemos garantizar que si existe el autoespacio asociado al autovalor 1, el mismo tenga dimension 1. Intuitivamente, esto se debe a que el grafo de la web puede tener varias componentes conexas. ¿Como comparamos sitios web que no están relacionados? Justamente la relación, ya sea directa o indirecta mediante transitividad me da algún tipo de relación de orden. Al no tener una relación de orden entre dos sitios web bien definida, es razonable que existan múltiples autovectores, es decir, rankings. Esto se puede ver claramente en la página 4 del paper de Bryan y Leise \cite{Bryan2006}.

Por lo tanto, la idea es básicamente buscar algún tipo de transformación relevante de la matriz de transición que me permita garantizar que no voy a tener \texttt{dangling nodes} y ademas que solo tenga una componente conexa, es decir, que el grafo resultante sea conexo. Definimos la siguiente matriz de transición, donde $v \in \mathbb{R}^{n \times n}$, con $v_i = 1 / n$ y $d \in \{0,1\}^n$,  $d_i = 1$ si $n_i = 0$ y $d_i = 0$ en caso contrario, como:

\begin{eqnarray*}
D & = & v d^t \\
P_1 & = & P + D.
\end{eqnarray*}

De esta manera, en caso de tener una página web que es un \texttt{dangling node}, le asignamos un link con probabilidad o peso uniforme a todos los otros sitios web $u \in Web$. Una interpretación equivalente es tomar a la matriz de transiciones como la matriz que describe una Cadena de Markov, donde el link pesado representa la probabilidad de dirigirse de una página a la otra. Por lo tanto, esta transformación se puede interpretar como que que existe una probabilidad uniforme de ir de uno de estos sitios a cualquiera de la web. Luego el famoso \texttt{navegante aleatorio} simplemente recorre el grafo utilizando estas probabiblidades.

Tambien podemos considerar la posibilidad de que el navegante aleatorio se dirija a una página web que no esta linkeada a la página a la que esta actualmente. Este fenómeno se conoce como teletransportación. Para incluirlo al modelo, tomemos un numero $c \in [0,1]$ y transformemos la matriz de transiciones de la siguiente manera, donde $\bar{1} \in \mathbb{R}^n$ es un vector tal que todos sus componentes valen 1:

\begin{eqnarray*}
E & = & v \bar{1}^t \\
P_2 & = & cP_1 + (1-c)E,
\end{eqnarray*}

Notar que en caso de tener $c=1$, estamos en la matriz de transición sin teletransportación. Por otro lado, si $c=0$ estamos en el caso donde solo hay teletransportación y no importa la estructura del grafo de la web al momento de calcular el ranking.

Notar que la elección de este parámetro $c$ no es trivial. En general, este parámetro se calcula haciendo análisis empíricos sobre los datos de navegadores y toolbars. Un trabajo interesante sobre este tema es el de Constantine et al. de Microsoft Research, donde utilizando los registros del toolbar de Internet Explorer y un modelo basado en la distribución Beta generalizan la matriz de transiciones para poder modelar un parámetro de teletransportación variable \cite{TeleParam}. Para este trabajo, simplemente utilizaremos la elección original constante del trabajo de Page \& Brin \cite{Brin1998} y fijaremos $\alpha = 0.85$.

\pagebreak

Esta nueva matriz de transición, dado que es estocástica por columnas y no tiene \texttt{dangling nodes}, nos garantiza que la dimension del autoespacio generado por el autovector de autovalor 1 es unitaria. Solo nos falta mostrar que todo autovector tiene todos sus elementos del mismo signo. Es facil probar la siguiente proposicion:

\begin{proposition}
Si la matriz M es positiva y estocástica por columnas, entonces todo autovector en $V_1(M)$ tiene todos sus elementos positivos o negativos.
\end{proposition}

Por lo tanto, ya probamos la existencia del autovector de norma 1 asociado al autovalor 1 de la matriz de transición transformada. El siguiente lema nos garantiza su unicidad. Su respectiva demostración se encuentra nuevamente en la página 7 del paper de Bryan y Leise \cite{Bryan2006}.

\begin{lemma}
\item Si M es positiva y estocástica por columnas, entonces $V_1(M)$ tiene dimension 1.
\end{lemma}

\begin{proposition}
Sea M una matriz real de n x n positiva y estocástica por columnas, y V el subespacio de $\mathbb{R}^n$ que consiste de aquellos vectores v tales que la suma de sus componentes sea 0, entonces $Mv \in V$ y $||Mv||_1 \leq \alpha||v||_1$ donde $\alpha = max_{1 \leq j \leq n}|1 - 2min_{1 \leq i \leq n}| < 1$.
\end{proposition}

\subsection{Tenis}

\subsubsection{Modelado}

El modelo GeM es un método de rankeo basado en PageRank para rankear equipos en ligas deportivas planteado por Govan et al. en "Generalizing Google's PageRank to Rank National Football League Teams" \cite{Govan2008}. Utilizaremos el mismo para analizar los rankings del ATP entre 1975 y 1977.
A continuación explicaremos como los autores modelan la matriz de transiciones que utiliza el algoritmo. Antes de comenzar con las definiciones, vale aclarar que para mantener la consistencia de la notación utilizada en este paper trabajaremos sobre la transpuesta de la matriz que se usa en el paper de Govan et al.

Se representa una temporada como un grafo dirigido con $n$ nodos. Cada nodo representa un participante o equipo y cada partido entre dos participantes representa una arista desde el perdedor al ganador igual a la diferencia de puntos obtenidos por cada participante en el partido.

La matriz $A^t$ de adyacencias queda definida de la siguiente manera, donde cada $w_{ij}$ representa la diferencia positiva del puntaje de cada participante en ese enfrentamiento.

\begin{equation}
A^t = \Bigg\{
  \begin{tabular}{ccc}
  $w_{ij}$ si el participante $j$ pierde con el participante $i$ \\
  $0$ en cualquier otro caso 
  \end{tabular}
\end{equation}

En caso de que un equipo pierda mas de una vez en el mismo torneo, será la suma de las diferencias de cada partido entre $i$ y $j$. Esta es la mayor generalización en cuanto a PageRank.

Luego se define la matriz H de la siguiente forma:
\begin{equation}
H = \Bigg\{
  \begin{tabular}{ccc}
  $w_{ij} / \sum\limits_{k=1}^n A^t_{kj} $ si hay un link de $j$ a $k$ \\
  0 en cualquier otro caso 
  \end{tabular}
\end{equation}

Finalmente definimos G a la matriz resultante de:

\begin{equation}
G = \alpha(A^t + au^t) + (1 - \alpha)ev^t
\end{equation}

Donde 0 $< \alpha < 1$, v es un vector de probabilidades, a es tal que $a_i$ es 1 si la fila $j$ de H es 0 y $a_i$ es 0 en cualquier otro caso y u sea un vector de probabilidad de $n$ x $1$. e se define como un vector fila con todas sus entradas igual a 1. Luego, el vector que contendrá los puntajes de cada equipo será un $\pi$ tal que: G$\pi$ = $\pi$.

Cada entrada $H_{ij}$ de H se puede interpretar como la probabilidad de que el participante $j$ pierda contra el participante $i$. Para los participantes invictos un simple ajuste que se propone es elegir un vector $u$ donde todas las entradas son $1/n$. Esto significa cambiar la probabilidad de los invictos a que puedan perder contra cualquiera de los otros participantes (incluido si mismo) con probabilidad uniforme.
El modelo básico utiliza $\alpha$ de la misma forma que en PageRank y $v^t$ como vector de personalización del sistema. Una simple elección puede ser $v$ = $(1/n)e$. Aunque $v$ ofrece mucha mas flexibilidad. Podría usarse con el resultado estadistico de un ranking previo, aumentando así las probabilidades de que cierto participante gane el presente torneo.
La elección del $\alpha$ determina la importancia de la matriz de personalización $ev^t$. A diferencia del caso de las paginas web, la elección de este parámetro puede ser difícil de medir empíricamente y en ultima instancia debe ser elegida por el organizador del torneo. En nuestro caso, decidimos elegir $\alpha = 0.85$.	 

\subsubsection{Sistema de puntos y consideraciones}

Para procesar los rankings, obtuvimos todos los partidos realizados entre 1975 y 1977 de todas las competencias a nivel internacional de tenis que sirven para clasificar al ATP \footnote{Los datos los obtuvimos de https://github.com/JeffSackmann/tennis\_atp, un repositorio con todos los rankings historicos de la ATP. La veracidad de los mismos es discutible, pero fue lo mejor que conseguimos dado que los investigadores que contactamos no nos quisieron proveer los datos: \say{Lamentablemente, toda esa información está hoy en pleno ida y vuelta legal con la ATP y no puedo bajo ningún concepto filtrarla.}}. (Australian Open, Indianapolis, Roma, Roland Garros, US Open, Wimbledon, Davis Cup, y muchos más) junto con los rankings de cada año. Uno a mitad de año y otro a finales de año. 

Los datos están completos a nivel enfrentamientos y participantes, pero carecen de los puntajes que cada jugador obtuvo por cada partido ganado, por avanzar de ronda y/o ganar un torneo.
Esta es una parte importante a aclarar del sistema de rankings de la ATP. Se obtienen puntos por cada partido ganado, ronda superada y torneo ganado, los cuales además varían por torneo en importancia, siendo los Grand Slam los de mayor jerarquía. Ademas, en esa época el sistema estaba basado en el puntaje promedio obtenido por el jugador.

Utiliza un sistema de defensa de puntos que fue variando con los años. En aquella época, la defensa se hacia por año. Es decir, los puntos realizados durante un año, se defienden al año siguiente. 

Igualmente esto no supone una limitación grave para computar los rankings y comprobar si Guillermo Vilas fue realmente o no 1ro en al menos uno de esos años.

Además, dada la naturaleza del comportamiento de PageRank, no solo importa a cuantos contrincantes derrotó un participante dado, si no a quienes derrotó. Ésta es la primera consideración a tener en cuenta y una carta a favor que será utilizada por el algoritmo. Dado que no disponemos del sistema de puntuación real y no sabemos a priori cuantos partidos jugó exactamente cada participante, es esencial que además de la puntuación que definamos por cada partido, un jugador obtenga un extra por el rango del rival derrotado. Esto mismo se puede interpretar como que ese jugador ganó un partido importante, dado que los partidos importantes son los de instancias decisivas (cuartos, semifinales o una final), y generalmente, al que solo llegan los mejores jugadores.    

Dadas estas consideraciones podemos definir un sistema de puntos con el que poder computar partidos. El mismo es muy sencillo. 3 puntos al ganador y 1 punto al perdedor.

Solo utilizaremos el ranking de fin de año, totalizando por los puntos acumulados en el mismo, obteniendo así un ranking por cada año.
 
Veremos luego en la experimentación, como este sistema resuelve el cálculo de los rankings de manera apropiada y explicaremos que es lo que sucede en cada año computado observando algunos detalles importantes y comparándolos con los rankings oficiales de la ATP para luego concluir si Vilas fue o no 1ro en algún momento entre 1975 y 1977 según PageRank.

\pagebreak
\input{potencia}

\subsection{Representación del grafo}

Ya hemos demostrado las condiciones necesarias para poder obtener el autovector asociado al autovalor dominante de una matriz de Markov. 
Ahora debemos proceder a calcular el mismo. Para esto, tenemos que tener en cuenta las cualidades del sistema y el método de resolución del algoritmo. Recordemos que en general, el grafo que representa la web tenderá a ser disconexo y muy grande, es decir, que podrán existir dos o mas rankings diferentes. Por lo tanto la matriz 
de transiciones puede ser muy esparsa e inclusive puede suceder que una página no tenga links de salida, dando lugar a dangling nodes. Para solucionar estos inconvenientes, con lo visto anteriormente disponemos de dos soluciones. Para los dangling nodes, la solución consiste en sumar una columna con probabilidad 1/n a la columna de ceros, esto en si, se puede interpretar como la probalidad de navegación aleatoria que previamente describimos. Aunque con esto no solucionamos el problema de la esparsidad de la matriz en si y el de poder tener mas de un ranking diferente. Para esto último, se agregó la matriz de probabilidad de teletransportación.

Dada esta definición, la matriz de transiciones resultante no es esparsa. 
Para sistemas muy grandes, esto puede resultar contraproducente a la hora de obtener el autovector asociado, dado que la complejidad espacial y temporal aumenta  considerablemente con la cantidad de información representada en la matriz. Sin embargo existe un resultado que podremos utilizar para mejorar la eficiencia del algoritmo en términos de complejidad temporal y espacial. El mismo se basa en la idea de Kamvar et al. \cite[Algoritmo 1]{Kamvar2003} para el calculo del autovector. Este resultado nos permite utilizar la matriz original de transiciones sin modificar en lo absoluto, pero si cambiando su representación, valiendonos de una buena estructura para almacenar las entradas de la misma. 

Las cualidades de la matriz hacen que sea razonable intentar pensar en una forma de representar solo las entradas que no sean ceros, y dado que la matriz suele ser esparsa, la misma contendrá muchos ceros que podrían no ser representados. Para esto optamos por una entre las 3 siguientes estructuras de representación:

\subsubsection{Dictionary of Keys ($DOK$)}

Consiste en un diccionario que mapea pares de fila-columa a la entrada. No se representan las entradas nulas. El formato es bueno para gradualmente construir una matriz esparsa en orden aleatorio, pero pobre para iterar sobre valores distintos de cero en orden lexicográfico. Uno construye típicamente una matriz en este formato y luego se convierte en otro formato más eficiente para su procesamiento.

\subsubsection{Compressed Sparse Row ($CSR$)}

Pone las entradas no nulas de las filas de la matriz en posiciones de memoria contiguas. Suponiendo que tenemos una matriz dispersa no simétrica, creamos vectores: uno para los números de punto flotante ($val$), y los otros dos para enteros ($col\_ind$, $row\_ptr$). El vector $val$ almacena los valores de los elementos distintos de cero de la matriz, de izquierda a derecha y de arriba hacia abajo. El vector $col\_ind$ almacena los índices de columna de los elementos en el vector $val$. Es decir, si $val(k) = a_ij$ entonces $col\_ind(k) = j$. El vector $row\_ptr$ almacena los lugares en el vector $val$ que comienza y termina una fila, es decir, si $val(k) = a_ij$ entonces $row_ptr(i) \leq k \leq row\_ptr(i+1)$. Por convención, se define $row\_ptr(n+1) = nnz$, en donde $nnz$ es el número de entradas no nulas en la matriz. Los ahorros de almacenamiento de este enfoque es significativo. En lugar de almacenar elementos $n^2$, solamente necesitamos $2nnz + n$ lugares de almacenamiento.

Veamos con un ejemplo como seria la representacion:
\\\\
$\hspace{3.2cm}\begin{pmatrix} 0 & 0 & 0 & 0 \\ 5 & 8 & 0 & 0 \\ 0 & 0 & 3 & 0 \\ 0 & 6 & 0 & 0 \\ \end{pmatrix}$
\\\\
Es una matrix de 4x4 con 4 entradas no nulas. Luego:\\
   
   $val$  = [ 5 8 3 6 ] \\
   $row\_ptr$ = [ 0 0 2 3 4 ] \\
   $col\_ind$ = [ 0 1 2 1 ] \\

\subsubsection{Compressed Sparse Column ($CSC$)}

La idea es análoga a $CSR$, pero la compresión se hace por columnas es decir, si $CSR$ comprime $A$, $CSC$ comprime $A^t$  

Sobre la matriz definida para $CSR$, con $CSC$ obtenemos lo siguiente: \\

   $val$  = [ 5 8 6 3 ] \\
   $col\_ptr$ = [ 0 1 3 4 4 ] \\   
   $row\_ind$ = [ 1 1 2 3 ] \\

Todos los resultados anteriores permiten evitar representar valores nulos.	
El motivo de nuestra elección se debe a que $CSR$ ofrece una buena representación de las filas de la matriz y es más eficiente a la hora de hacer operaciones del tipo A*x (matriz-vector) que es lo que nos interesa en el método de la potencia que realiza PageRank. $CSC$ en cambio, es efectiva para el producto x*A (vector-matriz) dado que la misma ofrece una mejor representación de las columnas. En contra partida, tanto $CSR$ como $CSC$, no permiten construcción incremental aleatoria, que si ofrece $DOK$, es decir, que cambios a la esparsidad de la matriz son costosos. En general están pensadas para ser estáticas, pero esto no es un inconveniente en nuestro caso, dado que no se realizaran cambios en la esparsidad de la matriz durante el proceso.

En el presente trabajo utilizaremos la idea de Kamvar et al. \cite[Algoritmo 1]{Kamvar2003} para el calculo del autovector valiendonos de nuestra estructura de representación elegida y compararemos los resultados con el algoritmo estandar para mostrar que al final de cuentas, si el sistema es muy grande y esparso, puede resultar muy beneficioso en términos de complejidad espacial y temporal.

\subsubsection{Elección de estructura}
En el caso de los eventos deportivos la elección de estructura no es tan relevante dado que la cantidad de equipos es acotada. Sin embargo, en el caso de las paginas web este factor es sumamente relevante dado que el uso de memoria de no representar el grafo de forma adecuada puede ser muy grande. De todas estas representaciones posibles, para este trabajo optamos por $CSR$ dado que es una representación eficiente para el producto de matrices esparsas, necesario para calcular de forma rápida el método de la potencia.