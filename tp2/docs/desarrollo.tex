\section{PageRank}

\subsection{Modelado para paginas web}

El algoritmo PageRank fue ideado en un principio para buscar de darle alguna medida de relevancia a los sitios web en internet. El mismo tiene dos interpretaciones equivalentes, que serán expuestas a continuación.

El problema se modela a partir de un grafo $G(Web,Links)$ donde $Web$ es el conjunto de sitios web y $Links$ es la cantidad de conexiones entre sitios. Consideremos que toda pagina web $u \in Web$ esta representada por un vértice y la relación entre paginas por un link con una arista. Una representación posible del grafo es mediante matrices de adyacencia. Definimos la matriz de adyacencia o conectividad $W \in \{0,1\}^{n \times n}$ de forma tal que $w_{ij} = 1$ si la pagina $j$ tiene un link a la pagina $i$ y $w_{ij} = 0$ en caso contrario. Por lo tanto, la cantidad de paginas a las que la pagina $u$ apunta ($d_{out}(u)$) se puede calcular como $n_j = \sum_{i=1}^{n} w_{ij}$.

\subsubsection{Propiedades}

Sea $x_j$ el puntaje asignado a la pagina o vértice $j \in Web$ y otra pagina $u \in Web$. La idea es buscar una medida que cumpla con las siguientes propiedades:
\begin{itemize}
  \item La relevancia de todo sitio web es positiva.
  \item La relevancia de un sitio web debe aumentar a medida que mas sitios unicos lo apuntan.
  \item La relevancia derivada de otro sitio web debe depender de su propia relevancia. Es decir, es mas valioso que me linkee un sitio relevante que uno no relevante. En caso de no cumplirse esta propiedad, el ranking seria fácilmente manipulable al permitir que un usuario cree muchos sitios que linkeen a uno para darle relevancia.
  \item La relevancia de todos los sitios web debe sumar uno. De esta manera estamos ante una distribución de probabilidad de los sitios. Mas adelante veremos que al interpretar esto mediante Cadenas de Markov existe una interpretación directa: la relevancia se puede ver como la proporción del tiempo total que un usuario pasa en ese sitio.
\end{itemize} 

Por lo tanto, estamos buscando una medida de relevancia tal que la importancia obtenida por la pagina $u$ obtenida por el link de la pagina $v$ sea proporcional a la relevancia de $v$ e inversamente proporcional al grado de $v$. El aporte del link de $v$ a $u$ entonces es $x_u = x_v / n_v$. Luego, sea $L_k \subseteq Web$ el conjunto de paginas que tienen un link a la pagina $k$. Por lo tanto, la relevancia total de un sitio sera:

\begin{eqnarray}
x_k = \sum_{j \in L_k} \frac{x_j}{n_j},~~~~k = 1,\dots,n. \label{eq:basicmodel}
\end{eqnarray}

Notar que esta es de cierta manera una definición recursiva. La relevancia de un sitio $u$ puede depender de la relevancia de un sitio $v$, y luego la de $v$ puede depender de la de $u$. A priori calcular la relevancia de un sitio puede parecer sumamente complicado, pero luego veremos que al plantearlo como un sistema de ecuaciones esta dificultad per se ya no se presenta.

Definimos entonces una matriz de transición o adyacencia con pesos en las aristas $P \in \mathbb{R}^{n \times n}$ tal que $p_{ij} = 1 / n_j$ si $w_{ij} = 1$ y $p_{ij} = 0$ en caso contrario. Luego, el modelo planteado en (\ref{eq:basicmodel}) para toda pagina web se puede expresar $Px = x$ donde $x \in \mathbb{R}^n$. Notar que esto es equivalente a encontrar el autovector de autovalor 1 tal que $x_i > 0$ y $\sum_{n=1}^{n} x_i = 1$. Notar que si logramos probar que bajo ciertas condiciones nuestra matriz de transición tiene autovalor 1, el signo de todos los elementos de un autovector es el mismo y la dimension del autoespacio es 1 ya tenemos un ranking valido. Esto se debe a que cualquier autovector puede ser reescalado a uno de norma unitaria con $x_i \geq 0$.

\pagebreak

\subsubsection{Existencia y Unicidad}

Bryan y Leise \cite{Bryan2006} analiza y prueba las condiciones bajo las que podemos garantizar que:
\begin{itemize}
\item La matriz de transición tiene autovalor 1.
\item La dimension del autoespacio asociado al autovalor 1 es 1. Es deseable que el ranking asociado a una matriz de transición sea único.
\item El signo de todos los elementos del autovector asociado al autovalor 1 es el mismo.
\end{itemize}

Veamos bajo que condiciones nuestra matriz de transición cumple con estas propiedades:

\begin{definition}
Una matriz cuadrada se llama estocástica por columnas si todos sus elementos son positivos y la suma de cada columna es igual a 1.
\end{definition}

A partir de esta definición se puede probar la siguiente proposición:
\begin{proposition}
Toda matriz estocástica por columnas tiene a 1 como autovalor.
\end{proposition}

Esto significa que si no existen \texttt{dangling nodes}, es decir, vértices con $d_{out} = 0$, podemos garantizar que nuestra matriz de transición es estocástica por columnas.

Notar que bajo las condiciones actuales no podemos garantizar que si existe el autoespacio asociado al autovalor 1, el mismo tenga dimension 1. Intuitivamente, esto se debe a que el grafo de la web puede tener varias componentes conexas.¿Como comparamos sitios web que no están relacionados? Justamente la relación, ya sea directa o indirecta mediante transitividad me da algún tipo de relación de orden. Al no tener una relación de orden entre dos sitios web bien definida, es razonable que existan múltiples autovectores, es decir, rankings. Esto se puede ver claramente en la pagina 4 del paper de Bryan y Leise \cite{Bryan2006}.

Por lo tanto, la idea es básicamente buscar algún tipo de transformación relevante de mi matriz de transición que me permita garantizar que no voy a tener \texttt{dangling nodes} y ademas que solo tenga una componente conexa, es decir, que el grafo sea conexo. Definimos la siguiente matriz de transición, donde $v \in \mathbb{R}^{n \times n}$, con $v_i = 1 / n$ y $d \in \{0,1\}^n$,  $d_i = 1$ si $n_i = 0$ y $d_i = 0$ en caso contrario, como:

\begin{eqnarray*}
D & = & v d^t \\
P_1 & = & P + D.
\end{eqnarray*}

De esta manera, en caso de tener una pagina web que es un \texttt{dangling node}, le asignamos un link uniforme a todos los sitios web $u \in Web$. Una interpretación equivalente es tomar a la matriz de transiciones como la matriz que describe una Cadena de Markov, donde el link pesado representa la probabilidad de dirigirse de una pagina a la otra. Por lo tanto, esta transformación se puede interpretar como que que existe una probabilidad uniforme de ir de uno de estos sitios a cualquiera de la web. Esto normalmente se conoce como el \texttt{navegante aleatorio}.

Tambien podemos considerar la posibilidad de que el navegante aleatorio se dirija a una pagina web que no esta linkeada a la pagina a la que esta actualmente. Este fenómeno se conoce como teletransportación. Para incluirlo al modelo, tomemos un numero $c \in [0,1]$ y transformemos la matriz de transiciones de la siguiente manera, donde $\bar{1} \in \mathbb{R}^n$ es un vector tal que todos sus componentes valen 1:

\begin{eqnarray*}
E & = & v \bar{1}^t \\
P_2 & = & cP_1 + (1-c)E,
\end{eqnarray*}

Notar que en caso de tener $c=1$, estamos en la matriz de transición sin teletransportación. Por otro lado, si $c=1$ estamos en el caso donde solo hay teletransportación y no importa la estructura del grafo de la web.

Esta nueva matriz de transición, dado que es estocástica por columnas y no tiene \texttt{dangling nodes}, nos garantiza que la dimension del autoespacio generado por el autovector de autovalor 1 es unitaria. Solo nos falta mostrar que todo autovector tiene todos sus elementos del mismo signo. Es facil probar la siguiente proposicion:

\begin{proposition}
Si la matriz M es positiva y estocástica por columnas, entonces todo autovector en $V_1(M)$ tiene todos sus elementos positivos o negativos.
\end{proposition}

Por lo tanto, ya probamos la existencia del autovector de norma 1 asociado al autovalor 1 de la matriz de transición transformada. El siguiente lema nos garantiza su unicidad. Su respectiva demostración se encuentra nuevamente en la pagina 7 del paper de Bryan y Leise \cite{Bryan2006}.

\begin{lemma}
\item Si M es positiva y estocástica por columnas, entonces $V_1(M)$ tiene dimension 1.
\end{lemma}

\begin{proposition}
Sea M una matriz real de n x n positiva y estocástica por columnas, y V el subespacio de $\mathbb{R}^n$ que consiste de aquellos vectores v tales que la suma de sus componentes sea 0, entonces $Mv \in V$ y $||Mv||_1 \leq \alpha||v||_1$ donde $\alpha = max_{1 \leq j \leq n}|1 - 2min_{1 \leq i \leq n}| < 1$.
\end{proposition}

\subsection{Modelado para Tenis}

El modelo GeM Ranking Method presente en el paper de Govan et al. plantea un método alternativo para rankear ligas deportivas. Utilizaremos el mismo para analizar los rankings del ATP entre 1975 y 1977. El algoritmo está motivado en el PageRank de Page y Brin, pero con unos ligeros retoques para adaptarlo a torneos y competencias.
En lo que sigue, explicaremos como se construye la matriz de transiciones que utiliza el algoritmo, pero antes de comenzar con las definiciones, vale aclarar que para mantener la homogeneidad haremos esta definición sobre la transpuesta de la matriz que se usa en el paper de Govan et al.

Se representa una temporada como un grafo directo con $n$ nodos. Cada nodo representa un participante o equipo y cada partido entre dos participantes representa una arista desde el perdedor al ganador igual a la diferencia positiva de los puntos obtenidos por cada participante en el partido.

La matriz $A^t$ de adyacencias queda definida de la siguiente manera: 
\\\\
$A^t$ = \Bigg\{
  \begin{tabular}{ccc}
  $w_{ij}$ si el participante $j$ pierde con el participante $i$ \\
  0 en cualquier otro caso 
  \end{tabular}
\\\\
Donde cada $w_{ij}$ representa la diferencia positiva del puntaje de cada participante en ese enfrentamiento. En caso de que un equipo pierda mas de una vez en el mismo torneo, será la suma de las diferencias de cada partido entre $i$ y $j$. Esta es la mayor generalización en cuanto a PageRank.

Luego se define la matriz H de la siguiente forma:
\\\\
H = \Bigg\{
  \begin{tabular}{ccc}
  $w_{ij} / \sum\limits_{k=1}^n A^t_{kj} $ si hay un link de $j$ a $k$ \\
  0 en cualquier otro caso 
  \end{tabular}

Luego definimos G a la matriz resultante de:
\\\\
G = $\alpha$($A^t$ + $au^t$) + $(1 - \alpha)ev^t$
\\\\
Donde 0 $< \alpha < 1$, v es un vector de probabilidades, a es tal que $a_i$ es 1 si la fila $j$ de H es 0 y $a_i$ es 0 en cualquier otro caso y u sea un vector de probabilidad de $n$ x $1$. e se define como un vector fila con todas sus entradas igual a 1.
\\\\
El vector que contendrá los puntajes de cada equipo será un $\pi$ tal que:
\\\\
G$\pi$ = $\pi$ 
\\\\
Cada entrada $H_{ij}$ de H se puede interpretar como la probabilidad de que el participante $j$ pierda contra el participante $i$. Para los participantes invictos un simple ajuste que se propone es elegir un vector $u$ donde todas las entradas son $1/n$. Esto significa cambiar la probabilidad de los invictos a que puedan perder contra cualquiera de los otros participantes (incluido si mismo) con probabilidad uniforme.
El modelo básico utiliza $\alpha$ de la misma forma que en PageRank y $v^t$ como vector de personalización del sistema. Una simple elección puede ser $v$ = $(1/n)e$. Aunque $v$ ofrece mucha mas flexibilidad. Podría usarse con el resultado estadistico de un ranking previo, aumentando así las probabilidades de que cierto participante gane el presente torneo.
La elección del $\alpha$ determina la importancia de la matriz de personalización $ev^t$. Una buena elección en el calculo de $\alpha$ podría ser la que se menciona en la siguiente investigación de microsoft research: \href{http://research.microsoft.com/apps/pubs/default.aspx?id=118374}{
Tracking the random surfer: Empirically measured teleportation parameters in PageRank} del cual mencionaremos la siguiente abstracción:

\begin{verbatim}
PageRank computes the importance of each page in a directed graph under a random
surfer model governed by a teleportation parameter. Commonly denoted alpha, 
this parameter models the probability of following an edge inside the graph or, 
when the graph comes from a network of web pages and links, clicking a link on a 
web page. We empirically measure the teleportation parameter based on browser 
toolbar logs and a click trail analysis. For a particular user or machine, 
such analysis produces a value of alpha. We find that these values nicely fit a 
Beta distribution with mean edge-following probability between 0.3 and 0.7, 
depending on the site. Using these distributions, we compute PageRank scores 
where PageRank is computed with respect to a distribution as the teleportation 
parameter, rather than a constant teleportation parameter. These new metrics are 
evaluated on the graph of pages in Wikipedia.
\end{verbatim}

Igualmente en nuestro caso computaremos los rankings con un $\alpha$ de 0.85 que es el utilizado en Govan et al. y que a su vez fue el standard de Google en sus comienzos.	 

\subsubsection{Sistema de puntos y consideraciones}

Para procesar los rankings, obtuvimos todos los partidos realizados entre 1975 y 1977 de todas las competencias a nivel internacional de tenis que sirven para clasificar al ATP. (Australian Open, Indianapolis, Roma, Roland Garros, US Open, Wimbledon, Davis Cup, y muchos más) junto con los rankings de cada año. Uno a mitad de año y otro a finales de año. 
\\
Los datos están completos a nivel enfrentamientos y participantes, pero carecen de los puntajes que cada jugador obtuvo por cada partido ganado, por avanzar de ronda y/o ganar un torneo.
Esta es una parte importante a aclarar del sistema de rankings de la ATP. Se obtienen puntos por cada partido ganado, ronda superada y torneo ganado, los cuales además varian por torneo en importancia, siendo los Grand Slam los de mayor jerarquía. 
Utiliza un sistema de defensa de puntos que fué variando con los años. En aquella epoca, la defensa se hacia por año. Es decir, los puntos realizados durante un año, se defienden al año siguiente. 

Igualmente esto no supone una limitación grave para computar los rankings y comprobar si Guillermo Vilas fue realmente o no 1ro al menos uno de esos años.
\\
Existe una relación directa entre sumar puntos y ganar partidos, y es que naturalmente el que más puntos tiene, más partidos ganó, naturalmente.
Además, dada la naturaleza del comportamiento de page rank, no solo importa a cuantos contrincantes derrotó un participante dado, si no a quienes derrotó. Ésta es la primera consideración a tener en cuenta y una carta a favor que será utilizada por el algoritmo. Dado que no disponemos del sistema de puntuación real y no sabemos a priori cuantos partidos jugó exactamente cada participante, es escencial que además de la puntuación que definamos por cada partido, un jugador obtenga un extra por el rango del rival derrotado. Esto mismo se puede interpretar como que ese jugador ganó un partido importante, dado que los partidos importantes son los de instancias decisivas (cuartos, semifinales o una final), y generalmente, al que solo llegan los mejores jugadores.    
\\
Dadas estas consideraciones podemos definir un sistema de puntos con el que poder computar partidos. El mismo es muy sencillo. 3 puntos al ganador y 1 punto al perdedor.
 Solo utilizaremos el ranking de fin de año, totalizando por los puntos acumulados en el mismo, obteniendo así 3 rankings por cada año.
\\
Veremos luego en la experimentación, como este sistema resuelve el cálculo de los rankings de manera apropiada y explicaremos que es lo que sucede en cada año computado observando algunos detalles importantes y comparandolos con los rankings oficiales de la ATP para luego concluir si Vilas fue o no, nuevamente, 1ro entre 1975 y 1977.

\subsection{Eliminacion Gausiana}

Esta seccion solo la pongo para que la consideres. Se podra hacer eliminacion gausiana con pivoteo para (P-I)x = 0? Igual si es posible es de orden cubico, con la web de millones de paginas se te va al carajo. Es solo para enriquecer la discusion.

\subsection{Representacion del grafo}

Ya hemos demostrado las condiciones necesarias para poder obtener el autovector asociado al autovalor dominante de una matriz de Markov. 
Ahora debemos proceder a calcular el mismo. Para esto, tenemos que tener en cuenta las cualidades del sistema y el método de resolución del algoritmo. Recordemos que en general, el grafo que representa la web tenderá a ser desconexo y muy grande, es decir, que podrán existir dos o mas rankings diferentes. Por lo tanto la matriz 
de transiciones puede ser muy esparsa e inclusive puede suceder que una página no tenga links de salida, dando lugar a dangling nodes. Para solucionar estos inconvenientes, con lo visto anteriormente disponemos de dos soluciones. Para los dangling nodes, la solución consiste en sumar una columna con probabilidad 1/n a la columna de ceros, esto en si, se puede interpretar como la probalidad de navegación aleatoria que previamente describimos. Aunque con esto no solucionamos el problema de la esparsidad de la matriz en si y el de poder tener mas de un ranking diferente. Para esto último, se agregó la matriz de probabilidad de teletransportación.

Dada esta definición, la matriz de transiciones resultante no es esparsa. 
Para sistemas muy grandes, esto puede resultar contraproducente a la hora de obtener el autovector asociado, dado que la complejidad espacial y temporal aumenta  considerablemente con la cantidad de información representada en la matriz. Sin embargo existe un resultado que podremos utilizar para mejorar la eficiencia del algoritmo en términos de complejidad temporal y espacial. El mismo se basa en la idea de Kamvar et al. \cite[Algoritmo 1]{Kamvar2003} para el calculo del autovector. Este resultado nos permite utilizar la matriz original de transiciones sin modificar en lo absoluto, pero si cambiando su representación, valiendonos de una buena estructura para almacenar las entradas de la misma. 

Las cualidades de la matriz hacen que sea razonable intentar pensar en una forma de representar solo las entradas que no sean ceros, y dado que la matriz suele ser esparsa, la misma contendrá muchos ceros que podrían no ser representados. Para esto optamos por una de entre las 3 siguientes estructuras de representación:

\begin{itemize}
\item Dictionary of Keys ($DOK$)

\item Compressed Sparse Row ($CSR$)

\item Compressed Sparse Column ($CSC$)
\end{itemize}

De todas estas representaciones posibles, para este t.p optamos por $CSR$. Aún así no haremos una elección sin una justificación apropiada del porque consideramos que es la mejor para nuestro trabajo, dado que como en toda estructura de datos, siempre existen pros y contras. Nos encargaremos en lo que sigue de exponer estos detalles para dejar en claro nuestro punto de vista. 

\begin{itemize}
\item Dictionary of Keys ($DOK$)
\end{itemize}
Consiste en un diccionario que mapea pares de fila-columa a la entrada. No se representan las entradas nulas. El formato es bueno para gradualmente construir una matriz esparsa en orden aleatorio, pero pobre para iterar sobre valores distintos de cero en orden lexicográfico. Uno construye típicamente una matriz en este formato y luego se convierte en otro formato más eficiente para su procesamiento.

\begin{itemize}
\item Compressed Sparse Row ($CSR$)
\end{itemize}
Pone las entradas no nulas de las filas de la matriz en posiciones de memoria contiguas. Suponiendo que tenemos una matriz dispersa no simétrica, creamos vectores: uno para los números de punto flotante ($val$), y los otros dos para enteros ($col\_ind$, $row\_ptr$). El vector $val$ almacena los valores de los elementos distintos de cero de la matriz, de izquierda a derecha y de arriba hacia abajo. El vector $col\_ind$ almacena los índices de columna de los elementos en el vector $val$. Es decir, si $val(k) = a_ij$ entonces $col\_ind(k) = j$. El vector $row\_ptr$ almacena los lugares en el vector $val$ que comienza y termina una fila, es decir, si $val(k) = a_ij$ entonces $row_ptr(i) \leq k \leq row\_ptr(i+1)$. Por convención, se define $row\_ptr(n+1) = nnz$, en donde $nnz$ es el número de entradas no nulas en la matriz. Los ahorros de almacenamiento de este enfoque es significativo. En lugar de almacenar elementos $n^2$, solamente necesitamos $2nnz + n$ lugares de almacenamiento.

Veamos con un ejemplo como seria la representacion:
\\\\
$\hspace{3.2cm}\begin{pmatrix} 0 & 0 & 0 & 0 \\ 5 & 8 & 0 & 0 \\ 0 & 0 & 3 & 0 \\ 0 & 6 & 0 & 0 \\ \end{pmatrix}$
\\\\
Es una matrix de 4x4 con 4 entradas no nulas. Luego:\\
   
   $val$  = [ 5 8 3 6 ] \\
   $row\_ptr$ = [ 0 0 2 3 4 ] \\
   $col\_ind$ = [ 0 1 2 1 ] \\

\begin{itemize}
\item Compressed Sparse Column ($CSC$)
La idea es analoga a $CSR$, pero la compresion se hace por columnas es decir, si $CSR$ comprime $A$, $CSC$ comprime $A^t$  
\end{itemize}

Sobre la matriz definida para $CSR$, con $CSC$ obtenemos lo siguiente: \\

   $val$  = [ 5 8 6 3 ] \\
   $col\_ptr$ = [ 0 1 3 4 4 ] \\   
   $row\_ind$ = [ 1 1 2 3 ] \\

Todos los resultados anteriores permiten evitar representar valores nulos.	
El motivo de nuestra elección se debe a que $CSR$ ofrece una buena representación de las filas de la matriz y es más eficiente a la hora de hacer operaciones del tipo A*x (matriz-vector) que es lo que nos interesa en el método de la potencia que realiza pageRank. $CSC$ en cambio, es efectiva para el producto x*A (vector-matriz) dado que la misma ofrece una mejor representación de las columnas. En contra partida, tanto $CSR$ como $CSC$, no permiten construcción incremental aleatoria, que si ofrece $DOK$, es decir, que cambios a la esparsidad de la matriz son costosos. En general están pensadas para ser estáticas, pero esto no es un inconveniente en nuestro caso, dado que no se realizaran cambios en la esparsidad de la matriz durante el proceso.

En el presente trabajo utilizaremos la idea de Kamvar et al. \cite[Algoritmo 1]{Kamvar2003} para el calculo del autovector valiendonos de nuestra estructura de representación elegida y compararemos los resultados con el algoritmo standard para mostrar que al final de cuentas, si el sistema es muy grande y esparso, puede resultar muy beneficioso en terminos de complejidad espacial y temporal.